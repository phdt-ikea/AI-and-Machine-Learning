"""
Dummy Dataset Preprocessing Pipeline

This script demonstrates a complete data preprocessing workflow using the
data_preprocessing module. It loads the dummy dataset generated by
generate_and_load_dummy_data.py and applies various preprocessing steps
to prepare it for machine learning models.

Preprocessing Steps
-------------------
1. Load the dummy dataset from CSV file
2. Handle missing values using mean imputation
3. Remove statistical outliers using Z-score method
4. Scale numeric features using standardization
5. Encode categorical variables using one-hot encoding
6. Save the preprocessed dataset

Input
-----
Requires 'dummy_dataset.csv' in the 'Data cleaning and preprocessing/' directory

Output
------
Creates 'cleaned_preprocessed_data.csv' with fully preprocessed data ready for modeling

Dependencies
------------
- data_preprocessing module (custom module with preprocessing functions)

Usage
-----
Run this script after generating the dummy dataset:
    $ python preprocess_dummy_dataset.py

Notes
-----
- Ensure dummy_dataset.csv exists before running this script
- The preprocessing pipeline is designed for demonstration purposes
- Adjust preprocessing steps based on specific data requirements
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler  # type: ignore
import data_preprocessing as dp # Import the data preprocessing module

# Load the dummy dataset
df_dummy = dp.load_data('Data cleaning and preprocessing/dummy_dataset.csv')

# Fill missing categorical data with the mode (most frequent value)
df_dummy['Category'].fillna(df_dummy['Category'].mode()[0], inplace=True)

# Handle missing values
df_dummy.fillna(df_dummy['Feature1'].mean(), inplace=True)

# Remove outliers from Feature1 only
feature1_mean = df_dummy['Feature1'].mean()
feature1_std = df_dummy['Feature1'].std()
z_scores = np.abs((df_dummy['Feature1'] - feature1_mean) / feature1_std)
df_dummy = df_dummy[z_scores < 3]

# Scale numeric features
scaler = StandardScaler()
df_dummy[df_dummy.select_dtypes(include=[np.number]).columns] = scaler.fit_transform(
    df_dummy.select_dtypes(include=[np.number]))

# Encode categorical variables
# One-hot encode the categorical feature
df_dummy = pd.get_dummies(df_dummy, columns=['Category'])

# Save the cleaned and preprocessed dataset
dp.save_data(df_dummy, 'Data cleaning and preprocessing/cleaned_preprocessed_data.csv')

print("Data preprocessing complete. Cleaned data saved to 'cleaned_preprocessed_data.csv'.")

# Ensure that all missing values have been handled.
print(df_dummy.isnull().sum())
# Confirm that outliers have been removed.
print(df_dummy.describe())
# Check that the numeric features have been scaled appropriately.
print(df_dummy.head())
# Ensure that categorical variables have been properly encoded.
print(df_dummy.columns)
